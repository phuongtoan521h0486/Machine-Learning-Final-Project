{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Breast Cancer Wisconsin (Diagnostic) dataset is a classic binary classification dataset in machine learning. It contains features computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. The goal is to predict whether the mass is benign (non-cancerous) or malignant (cancerous)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Breast Cancer dataset\n",
    "breast_cancer = load_breast_cancer()\n",
    "X, y = breast_cancer.data, breast_cancer.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sigmoid and softmax activation functions and their derivatives\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def softmax(x):\n",
    "    exps = np.exp(x - np.max(x))\n",
    "    return exps / np.sum(exps, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights and biases\n",
    "input_size = X.shape[1]\n",
    "hidden_size = 20\n",
    "output_size = 2\n",
    "learning_rate = 0.1\n",
    "\n",
    "weights_input_hidden = np.random.randn(input_size, hidden_size)\n",
    "biases_input_hidden = np.zeros((1, hidden_size))\n",
    "\n",
    "weights_hidden_output = np.random.randn(hidden_size, output_size)\n",
    "biases_hidden_output = np.zeros((1, output_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.8100, Accuracy: 0.7105\n",
      "Epoch [2/100], Loss: 0.6829, Accuracy: 0.7632\n",
      "Epoch [3/100], Loss: 0.5932, Accuracy: 0.8070\n",
      "Epoch [4/100], Loss: 0.5290, Accuracy: 0.8333\n",
      "Epoch [5/100], Loss: 0.4820, Accuracy: 0.8509\n",
      "Epoch [6/100], Loss: 0.4466, Accuracy: 0.8684\n",
      "Epoch [7/100], Loss: 0.4192, Accuracy: 0.8772\n",
      "Epoch [8/100], Loss: 0.3973, Accuracy: 0.8860\n",
      "Epoch [9/100], Loss: 0.3794, Accuracy: 0.8860\n",
      "Epoch [10/100], Loss: 0.3645, Accuracy: 0.8947\n",
      "Epoch [11/100], Loss: 0.3518, Accuracy: 0.8947\n",
      "Epoch [12/100], Loss: 0.3408, Accuracy: 0.9035\n",
      "Epoch [13/100], Loss: 0.3311, Accuracy: 0.9123\n",
      "Epoch [14/100], Loss: 0.3224, Accuracy: 0.9123\n",
      "Epoch [15/100], Loss: 0.3146, Accuracy: 0.9123\n",
      "Epoch [16/100], Loss: 0.3075, Accuracy: 0.9123\n",
      "Epoch [17/100], Loss: 0.3010, Accuracy: 0.9123\n",
      "Epoch [18/100], Loss: 0.2950, Accuracy: 0.9211\n",
      "Epoch [19/100], Loss: 0.2895, Accuracy: 0.9211\n",
      "Epoch [20/100], Loss: 0.2843, Accuracy: 0.9298\n",
      "Epoch [21/100], Loss: 0.2794, Accuracy: 0.9298\n",
      "Epoch [22/100], Loss: 0.2749, Accuracy: 0.9298\n",
      "Epoch [23/100], Loss: 0.2706, Accuracy: 0.9298\n",
      "Epoch [24/100], Loss: 0.2665, Accuracy: 0.9298\n",
      "Epoch [25/100], Loss: 0.2627, Accuracy: 0.9298\n",
      "Epoch [26/100], Loss: 0.2591, Accuracy: 0.9298\n",
      "Epoch [27/100], Loss: 0.2556, Accuracy: 0.9386\n",
      "Epoch [28/100], Loss: 0.2523, Accuracy: 0.9386\n",
      "Epoch [29/100], Loss: 0.2492, Accuracy: 0.9386\n",
      "Epoch [30/100], Loss: 0.2462, Accuracy: 0.9386\n",
      "Epoch [31/100], Loss: 0.2433, Accuracy: 0.9386\n",
      "Epoch [32/100], Loss: 0.2405, Accuracy: 0.9386\n",
      "Epoch [33/100], Loss: 0.2379, Accuracy: 0.9386\n",
      "Epoch [34/100], Loss: 0.2353, Accuracy: 0.9386\n",
      "Epoch [35/100], Loss: 0.2328, Accuracy: 0.9386\n",
      "Epoch [36/100], Loss: 0.2305, Accuracy: 0.9386\n",
      "Epoch [37/100], Loss: 0.2282, Accuracy: 0.9474\n",
      "Epoch [38/100], Loss: 0.2260, Accuracy: 0.9474\n",
      "Epoch [39/100], Loss: 0.2239, Accuracy: 0.9474\n",
      "Epoch [40/100], Loss: 0.2218, Accuracy: 0.9474\n",
      "Epoch [41/100], Loss: 0.2198, Accuracy: 0.9474\n",
      "Epoch [42/100], Loss: 0.2179, Accuracy: 0.9561\n",
      "Epoch [43/100], Loss: 0.2160, Accuracy: 0.9561\n",
      "Epoch [44/100], Loss: 0.2142, Accuracy: 0.9561\n",
      "Epoch [45/100], Loss: 0.2124, Accuracy: 0.9561\n",
      "Epoch [46/100], Loss: 0.2107, Accuracy: 0.9561\n",
      "Epoch [47/100], Loss: 0.2090, Accuracy: 0.9561\n",
      "Epoch [48/100], Loss: 0.2074, Accuracy: 0.9561\n",
      "Epoch [49/100], Loss: 0.2058, Accuracy: 0.9561\n",
      "Epoch [50/100], Loss: 0.2043, Accuracy: 0.9561\n",
      "Epoch [51/100], Loss: 0.2028, Accuracy: 0.9561\n",
      "Epoch [52/100], Loss: 0.2014, Accuracy: 0.9561\n",
      "Epoch [53/100], Loss: 0.1999, Accuracy: 0.9561\n",
      "Epoch [54/100], Loss: 0.1985, Accuracy: 0.9561\n",
      "Epoch [55/100], Loss: 0.1972, Accuracy: 0.9561\n",
      "Epoch [56/100], Loss: 0.1959, Accuracy: 0.9561\n",
      "Epoch [57/100], Loss: 0.1946, Accuracy: 0.9561\n",
      "Epoch [58/100], Loss: 0.1933, Accuracy: 0.9561\n",
      "Epoch [59/100], Loss: 0.1920, Accuracy: 0.9561\n",
      "Epoch [60/100], Loss: 0.1908, Accuracy: 0.9561\n",
      "Epoch [61/100], Loss: 0.1896, Accuracy: 0.9561\n",
      "Epoch [62/100], Loss: 0.1885, Accuracy: 0.9561\n",
      "Epoch [63/100], Loss: 0.1873, Accuracy: 0.9561\n",
      "Epoch [64/100], Loss: 0.1862, Accuracy: 0.9474\n",
      "Epoch [65/100], Loss: 0.1851, Accuracy: 0.9474\n",
      "Epoch [66/100], Loss: 0.1841, Accuracy: 0.9474\n",
      "Epoch [67/100], Loss: 0.1830, Accuracy: 0.9474\n",
      "Epoch [68/100], Loss: 0.1820, Accuracy: 0.9474\n",
      "Epoch [69/100], Loss: 0.1810, Accuracy: 0.9474\n",
      "Epoch [70/100], Loss: 0.1800, Accuracy: 0.9474\n",
      "Epoch [71/100], Loss: 0.1790, Accuracy: 0.9474\n",
      "Epoch [72/100], Loss: 0.1780, Accuracy: 0.9474\n",
      "Epoch [73/100], Loss: 0.1771, Accuracy: 0.9474\n",
      "Epoch [74/100], Loss: 0.1762, Accuracy: 0.9474\n",
      "Epoch [75/100], Loss: 0.1752, Accuracy: 0.9474\n",
      "Epoch [76/100], Loss: 0.1744, Accuracy: 0.9474\n",
      "Epoch [77/100], Loss: 0.1735, Accuracy: 0.9474\n",
      "Epoch [78/100], Loss: 0.1726, Accuracy: 0.9474\n",
      "Epoch [79/100], Loss: 0.1717, Accuracy: 0.9474\n",
      "Epoch [80/100], Loss: 0.1709, Accuracy: 0.9474\n",
      "Epoch [81/100], Loss: 0.1701, Accuracy: 0.9474\n",
      "Epoch [82/100], Loss: 0.1693, Accuracy: 0.9474\n",
      "Epoch [83/100], Loss: 0.1685, Accuracy: 0.9474\n",
      "Epoch [84/100], Loss: 0.1677, Accuracy: 0.9561\n",
      "Epoch [85/100], Loss: 0.1669, Accuracy: 0.9561\n",
      "Epoch [86/100], Loss: 0.1661, Accuracy: 0.9561\n",
      "Epoch [87/100], Loss: 0.1654, Accuracy: 0.9561\n",
      "Epoch [88/100], Loss: 0.1646, Accuracy: 0.9561\n",
      "Epoch [89/100], Loss: 0.1639, Accuracy: 0.9561\n",
      "Epoch [90/100], Loss: 0.1632, Accuracy: 0.9561\n",
      "Epoch [91/100], Loss: 0.1625, Accuracy: 0.9561\n",
      "Epoch [92/100], Loss: 0.1618, Accuracy: 0.9561\n",
      "Epoch [93/100], Loss: 0.1611, Accuracy: 0.9561\n",
      "Epoch [94/100], Loss: 0.1604, Accuracy: 0.9561\n",
      "Epoch [95/100], Loss: 0.1597, Accuracy: 0.9561\n",
      "Epoch [96/100], Loss: 0.1590, Accuracy: 0.9561\n",
      "Epoch [97/100], Loss: 0.1584, Accuracy: 0.9561\n",
      "Epoch [98/100], Loss: 0.1577, Accuracy: 0.9561\n",
      "Epoch [99/100], Loss: 0.1571, Accuracy: 0.9561\n",
      "Epoch [100/100], Loss: 0.1565, Accuracy: 0.9561\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    # Forward pass\n",
    "    # Input to hidden layer\n",
    "    hidden_input = np.dot(X_train, weights_input_hidden) + biases_input_hidden\n",
    "    hidden_output = sigmoid(hidden_input)\n",
    "    \n",
    "    # Hidden to output layer\n",
    "    output_input = np.dot(hidden_output, weights_hidden_output) + biases_hidden_output\n",
    "    output = softmax(output_input)\n",
    "    \n",
    "    # Backpropagation\n",
    "    # Compute gradients\n",
    "    d_output = output - np.eye(output_size)[y_train]\n",
    "    d_output /= len(X_train)\n",
    "    \n",
    "    d_weights_hidden_output = np.dot(hidden_output.T, d_output)\n",
    "    d_biases_hidden_output = np.sum(d_output, axis=0, keepdims=True)\n",
    "    \n",
    "    d_hidden = np.dot(d_output, weights_hidden_output.T) * sigmoid_derivative(hidden_output)\n",
    "    \n",
    "    d_weights_input_hidden = np.dot(X_train.T, d_hidden)\n",
    "    d_biases_input_hidden = np.sum(d_hidden, axis=0, keepdims=True)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    weights_input_hidden -= learning_rate * d_weights_input_hidden\n",
    "    biases_input_hidden -= learning_rate * d_biases_input_hidden\n",
    "    weights_hidden_output -= learning_rate * d_weights_hidden_output\n",
    "    biases_hidden_output -= learning_rate * d_biases_hidden_output\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = -np.sum(np.eye(output_size)[y_train] * np.log(output)) / len(X_train)\n",
    "    \n",
    "    # Compute accuracy\n",
    "    hidden_output_test = sigmoid(np.dot(X_test, weights_input_hidden) + biases_input_hidden)\n",
    "    output_test = softmax(np.dot(hidden_output_test, weights_hidden_output) + biases_hidden_output)\n",
    "    predicted = np.argmax(output_test, axis=1)\n",
    "    accuracy = np.mean(predicted == y_test)\n",
    "    \n",
    "    if (epoch) < 100:\n",
    "        print(f'Epoch [{epoch+1}/100], Loss: {loss:.4f}, Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy test: 0.9561\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "# Forward pass\n",
    "hidden_input = np.dot(X_test, weights_input_hidden) + biases_input_hidden\n",
    "hidden_output = sigmoid(hidden_input)\n",
    "\n",
    "output_input = np.dot(hidden_output, weights_hidden_output) + biases_hidden_output\n",
    "output = softmax(output_input)\n",
    "\n",
    "predicted = np.argmax(output, axis=1)\n",
    "accuracy = np.mean(predicted == y_test)\n",
    "print(f'Accuracy test: {accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
